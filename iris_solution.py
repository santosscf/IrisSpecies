# -*- coding: utf-8 -*-
"""Iris_Solution

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QlBw3jdrViho1d5H_kU3NYI3u6fBrgxI
"""

# INSTITUTO FEDERAL DO AMAZONAS - CMZL
#   Professor: Carlos Mar
#       Curso: Introdução a Aprendizagem de Máquina
#       Aluna: Stefany Caroline Ferreira dos Santos

#   Atividade: Implementar o modelo de aprendizagem supervisionada Naive Bayes 
#              para a base de dados Espécies de Iris
#   Descrição: Classifica as espécies de Iris utilizando 3 modelos, o primeiro 
#              sem uso de bibliteca, o segundo usando a função Gaussiana e o 
#              terceiro usando a função de Bernoulli
# Última mod.: 03/12/2021

# Bibliotecas
import random
random.seed(42)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB

# Carrega os dados
dados = pd.read_csv("https://raw.githubusercontent.com/santosscf/IrisSpecies/main/Dataset/Iris.csv")

# Remove linhas com elementos nulos
dados = dados.dropna(axis="rows")

# Armazena os nomes das classes e atributos
classes = np.array(pd.unique(dados[dados.columns[-1]]), dtype=str)
atributos = list(dados.columns)

# Imprime os dados
print("Número de linhas e colunas na matriz de atributos:", dados.shape)
dados.head()

# Transforma os dados para a estrutura numpy
dados = dados.to_numpy()
nrow,ncol = dados.shape
y = dados[:,-1]
X = dados[:,0:ncol-1]

# Seleciona os conjuntos de teste e treinamento
X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, train_size = 0.7, random_state = 42)

# Implementa o método de classificação Naive Bayes
    # Função para calcular a verossimilhança
def likelyhood(y, Z):
    def gaussian(x, mu, sig):
        return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))
    prob = 1
    for j in np.arange(0, Z.shape[1]):
        m = np.mean(Z[:,j])
        s = np.std(Z[:,j])      
        prob = prob*gaussian(y[j], m, s)
    return prob

    # Calcula a probabilidade de pertencer a classe
P = pd.DataFrame(data=np.zeros((X_teste.shape[0], len(classes))), columns = classes) 
for i in np.arange(0, len(classes)):
    elements = tuple(np.where(y_treino == classes[i]))
    Z = X_treino[elements,:][0]
    for j in np.arange(0,X_teste.shape[0]):
        x = X_teste[j,:]
        pj = likelyhood(x,Z)
        priori = len(elements)/X_treino.shape[0]
        P[classes[i]][j] = pj*priori

P.head()

# Decisão Baysiana
y_pred = []
for i in np.arange(0, P.shape[0]):
    c = np.argmax(np.array(P.iloc[[i]]))
    y_pred.append(P.columns[c])
y_pred = np.array(y_pred, dtype=str)

score = accuracy_score(y_pred, y_teste)
print('Precisão:', score)

# Modelagem: classificador Naive Bayses usando a biblioteca scikit-learn (Função Gaussiana)
modelo_Gau = GaussianNB()
modelo_Gau.fit(X_treino, y_treino)

y_pred_Gau = modelo_Gau.predict(X_teste)
score_Gau = accuracy_score(y_pred_Gau, y_teste)
print('Precisão:', score_Gau)

# Modelagem: classificador Naive Bayses usando a biblioteca scikit-learn (Função de Bernoulli)
modelo_Ber = BernoulliNB()
modelo_Ber.fit(X_treino, y_treino)

y_pred_Ber = modelo_Ber.predict(X_teste)
score_Ber = accuracy_score(y_pred_Ber, y_teste)
print('Accuracy:', score_Ber)